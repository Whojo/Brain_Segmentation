{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPITA 2022 MLRF practice_06-01_mri_seg v2022-06-30_130152 by Nicolas BOUTRY and Joseph CHAZALON\n",
    "\n",
    "<div style=\"overflow: auto; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"Creative Commons License\" src='img/CC-BY-4.0.png' style='float: left; margin-right: 20px'>\n",
    "    \n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 06 part 01: Image segmentation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We will demonstrate how to leverage image description and classification techniques you have learned to create a detector of tumoral regions in MRI brain scans.\n",
    "\n",
    "As this is the last session of this course, you will be given less instructions than before.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "It consists in building a **pixel** classifier, as opposed to the **image** classifier that you previously implemented, to assign a value (here a binary value) to **each pixel** to indicate whether the area described by this pixel is tumoral or not.\n",
    "\n",
    "The dataset we provide is composed of *slices* of brain MRIs from different patients, each slice having 4 *modalities*, ie 4 acquisitions using 4 different tools.\n",
    "\n",
    "The ground truth for each slice (and the segmentation you should produce for grading) indicates for each pixel whether or not it covers a tumoral regions:\n",
    "- `0` means **no tumor**;\n",
    "- `1` means **tumor**.\n",
    "\n",
    "We encourage you to read the `README.md` accompanying the dataset distribution for further details.\n",
    "\n",
    "**You will have to predict exactly the same values (either `0` or `1`) for each pixel of the test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and expected outputs\n",
    "This project is based on a scientific challenge which aimed at detecting specific parts of tumoral regions, in full 3D MRIs, as illustrated below: (a), (b) and (c) images show different modalities for the same brain (bottom) and the kind of tumoral region which can be detected (top), and (d) show the aggregated prediction.\n",
    "![Original challenge](img/practice_06/brain_tumor_types.png)\n",
    "\n",
    "\n",
    "In our case, we will only consider isolated **2D slices** and predict a **binary label** (tumoral/sane).\n",
    "Here is an illustration of some preliminary results we obtained on the segmentation task you have to complete – the first 4 columns are the original data, the 5th is the predicted segmentation, and the last is the expected segmentation:\n",
    "![Session sample results](img/practice_06/sample_results.png)\n",
    "\n",
    "#### Preprecessing and normalization\n",
    "![](img/warning.png)\n",
    "\n",
    "We manually extracted slices from the dataset, and the full volume is required to properly normalize each acquisition.  \n",
    "As a result, **we already normalized the slices** (centered and reduced) so you do not have to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "We **suggest** you proceed in 12 steps, but most of then can be skipped (you should at least train one non linear classifier) if your goal is to build and validate a first version of your pipeline:\n",
    "\n",
    "1. Load resources\n",
    "2. Prepare training data, pixel-level predictions\n",
    "3. Create, train and validate a linear, pixel-level classifier (**can be skipped**)\n",
    "4. Use the appropriate performance metric\n",
    "5. Create, train and validate a non-linear SVM classifier\n",
    "6. Multi-layer Perceptron (**can be skipped**)\n",
    "7. Random Forests (**can be skipped**)\n",
    "8. Use more context for classification (**can be skipped**)\n",
    "9. Post-process the results (**can be skipped**)\n",
    "10. Train on complete dataset (**can be skipped**)\n",
    "11. Process the test set\n",
    "12. Export your results and submit them for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "The resources for this session are stored in the student's archive you obtained separately.\n",
    "\n",
    "Here is a description of the files we provide in the student's archive:\n",
    "- `train_data_x.dat`: 256 normalized slices, one per patient,\n",
    "  containing 240x240 images with 4 channels (1 for each modality) in float32,\n",
    "  to be used for training.\n",
    "- `train_data_y.dat`: 256 target segmentations, one per patient,\n",
    "  containing 240x240 images with 1 channel (indicating tumor or clean region) in uint8,\n",
    "  to be used for training.\n",
    "- `test_data_x.dat`: 29 normalized slices, one per patient (not in the training set),\n",
    "  containing 240x240 images with 4 channels (1 for each modality) in float32,\n",
    "  to be used for testing. **Ground truth not provided.**\n",
    "\n",
    "The `.dat` files we use here are binary dumps of NumPy arrays exactly as they are\n",
    "represented in memory.\n",
    "Each file contains exactly one array.\n",
    "\n",
    "They are meant to be \"[memmapped](https://en.wikipedia.org/wiki/Memory-mapped_file)\"\n",
    "because it is very useful for large files — and the original dataset is large —\n",
    "as it avoids to load the complete array in RAM, but instead caches pages in RAM and\n",
    "loads content from disk on demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Load resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "seed = 42 # For reproductability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Make sure the resources location below is correct.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the resources location\n",
    "PATH_TO_RESOURCES = Path(\"./student/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Use `np.memmap(...)` and the `README.md` file provided with the dataset to load your data.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 240, 240, 4), (256, 240, 240), (29, 240, 240, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO load the resources\n",
    "def mmap_load(filename, *, dtype = np.float32, shape: tuple):\n",
    "    return np.memmap(PATH_TO_RESOURCES / filename, mode=\"r\", dtype=dtype, shape=shape)\n",
    "\n",
    "train_data_x = mmap_load(\"train_data_x.dat\", shape=(256, 240, 240, 4))\n",
    "train_data_y = mmap_load(\"train_data_y.dat\", shape=(256, 240, 240), dtype=np.uint8)\n",
    "test_data_x  = mmap_load(\"test_data_x.dat\",  shape=(29, 240, 240, 4))\n",
    "\n",
    "train_data_x.shape, train_data_y.shape, test_data_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Make sure you understand how your dataset is structured. Display some slices from the training set and their associated ground truth. As you will do this often, define a simple function to show slices, segmentations, etc.**\n",
    "\n",
    "*Hint: this function should display something like this:*\n",
    "    \n",
    "![Session sample results](img/practice_06/sample_results.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d557fc68b58416d8affcb90511eb567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=127, description='i', max=255), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "def display_slice(x: np.array, *, pred: np.array = None, seg: np.array = None, chan_names: list[str] = [\"T1\", \"T1ce\", \"T2\", \"FLAIR\", \"PRED\", \"SEG\"]) -> None:\n",
    "    \"\"\"\n",
    "    Display the content of a slice `x`, the optional predicted segmentation `pred`, and the optional target segmentation `seg`.\n",
    "   \n",
    "    Arguments\n",
    "    ---------\n",
    "    x: np.array of shape (rows, cols, channels) and dtype np.float\n",
    "        Input data (slice with 4 modalities, typically).\n",
    "  \n",
    "    pred: np.array of shape (rows, cols) and dtype np.float\n",
    "        Predicted values for each pixel.\n",
    "  \n",
    "    seg: np.array of shape (rows, cols) and dtype np.float\n",
    "        Target values for each pixel.\n",
    "        \n",
    "    chan_names: list of strings containing the name for the title\n",
    "        of each channel in `x`.\n",
    "    \"\"\"\n",
    "    _, _, nb_channels = x.shape\n",
    "    ncols = nb_channels\n",
    "    if (pred is not None):\n",
    "        ncols += 1\n",
    "    if (seg is not None):\n",
    "        ncols += 1\n",
    "\n",
    "    _, axis_arr = plt.subplots(ncols=ncols, figsize=(5 * ncols, 5))\n",
    "    \n",
    "    for chan_i in range(nb_channels):\n",
    "        axis_arr[chan_i].imshow(x[:, :, chan_i])\n",
    "        axis_arr[chan_i].axis(\"off\")\n",
    "        axis_arr[chan_i].set_title(chan_names[chan_i])\n",
    "\n",
    "    \n",
    "    if (pred is not None):\n",
    "        chan_i += 1\n",
    "        axis_arr[chan_i].imshow(pred)\n",
    "        axis_arr[chan_i].axis(\"off\")\n",
    "        axis_arr[chan_i].set_title(chan_names[chan_i])\n",
    "        \n",
    "    if (seg is not None):\n",
    "        chan_i += 1\n",
    "        axis_arr[chan_i].imshow(seg)\n",
    "        axis_arr[chan_i].axis(\"off\")\n",
    "        axis_arr[chan_i].set_title(chan_names[chan_i])\n",
    "\n",
    "\n",
    "def interact_display(train_data_x: np.array, train_data_y: np.array, train_data_y_pred: np.array = None, *, chan_names: list[str] = [\"T1\", \"T1ce\", \"T2\", \"FLAIR\", \"PRED\", \"SEG\"]):\n",
    "    \"\"\"\n",
    "    Display an interactive visualisation off `train_data_x` and `train_data_y`.\n",
    "    \"\"\"\n",
    "    @interact(i=(0, train_data_x.shape[0] - 1))\n",
    "    def closure_display(i):\n",
    "        if (train_data_y_pred is None):\n",
    "            display_slice(train_data_x[i, ...], seg=train_data_y[i, ...], chan_names=chan_names)\n",
    "        else:\n",
    "            display_slice(train_data_x[i, ...], seg=train_data_y[i, ...], pred=train_data_y_pred[i, ...], chan_names=chan_names)\n",
    "        \n",
    "interact_display(train_data_x, train_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare training data, pixel-level predictions\n",
    "Your goal is to train a pixel-level classifier, ie to create a set of pixels samples along with the expected prediction for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Select some slices for training\n",
    "You do not want to validate your complete pipeline by training on the full dataset because this would be way too long.\n",
    "\n",
    "We recommend to select slices (which all belong to different patients) from the original data.\n",
    "\n",
    "**Why don't we take all the pixels and split the pixels into two sets globally?**\n",
    "Because we would not appropriately test the generalization capabilities of our classifier, as some test samples would belong to slides from which we took pixels for training. We would have a high correlation between train and test sets.\n",
    "\n",
    "Also, do not forget to compute the target values for each slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Select some slices for training, and leave some out for validation. We used either 3 or 10 slices in our preliminary tests.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "nb_train_sample = 10\n",
    "nb_val_sample = 10\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "sample_id = rng.choice(train_data_x.shape[0], size=(nb_train_sample + nb_val_sample), replace=False)\n",
    "\n",
    "sample_train_id = sample_id[:nb_train_sample]\n",
    "sample_val_id = sample_id[nb_train_sample:]\n",
    "\n",
    "train_data_x_sample = train_data_x[sample_train_id]\n",
    "train_data_x_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Select the target values for each of the slices you previously selected.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "train_data_y_sample = train_data_y[sample_train_id]\n",
    "train_data_y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**To make sure your selection procedure is correct, you should display the slices you selected and the associated ground truth.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae8e99a63264ebba535c4c6203f1250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this should display some slices with the expected output\n",
    "interact_display(train_data_x_sample, train_data_y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Select some slices for validation\n",
    "This is also optional but makes it much easier to validate the pipeline.\n",
    "\n",
    "**Note that those slices should not be present in the training set.**\n",
    "\n",
    "Do not forget to compute the target values for each slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Select some slices which do not belong to the sample we just created, and create a small validation set. We used either 3 or 10 slices in our preliminary tests.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "val_data_x_sample = train_data_x[sample_val_id]\n",
    "val_data_x_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "val_data_y_sample = train_data_y[sample_val_id]\n",
    "val_data_y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Select the pixel from the brain (exclude the background)\n",
    "**Important: adding the background pixels (the pixels where there is no information) strongly perturbs the training.**\n",
    "\n",
    "Because we had to normalize the slices before giving them to you (normalization can only be done using the full 3D scan), we provide you with a function `slices_to_masks` which computes the masks for a stack of slices.\n",
    "\n",
    "For each slices (with 4 modalities), it computes a mask as illustrated below:\n",
    "![Mask example](img/practice_06/mask_example.png)\n",
    "\n",
    "**You must use this mask to select only the pixels falling within the brain for training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slices_to_masks(slices):\n",
    "    '''\n",
    "    Computes the foreground masks for each slice in a stack.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    slices: numpy.array, shape (NUM_SLICES, 240, 240, 4)\n",
    "        an array of slices for which every pixel belonging to the background has \n",
    "        the smallest possible value in each modality.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    slices_masks: numpy.array, shape (NUM_SLICES, 240, 240), dtype numpy.bool\n",
    "        the maks for each of the original slices indicating the foreground pixels.\n",
    "\n",
    "    '''\n",
    "    if slices.ndim != 4:\n",
    "        raise ValueError(\"slices_to_masks expects as parameter an array of 4 dimensions (numslices, rows, cols, features).\"\n",
    "                         \" It got instead an array of %s dimensions\" % (slices.ndim, ))\n",
    "    slices_min = np.min(slices, axis=(1, 2))\n",
    "    slices_masks = np.all(slices > slices_min[:, np.newaxis, np.newaxis, :],\n",
    "                          axis=-1)\n",
    "    return slices_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_train_data_sample = slices_to_masks(train_data_x_sample)\n",
    "mask_val_data_sample = slices_to_masks(val_data_x_sample)\n",
    "mask_train_data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15796a7b030431b8ccaed89e57264d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact_display(train_data_x_sample, mask_train_data_sample, chan_names=[\"T1\", \"T1ce\", \"T2\", \"FLAIR\", \"MASK\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Format the training data appropriately using masks\n",
    "The goal is to obtain two arrays:\n",
    "- a `x` variant with shape `(number_of_pixels, number_of_features_per_pixel)`, ie `(BIG_NUMBER, 4)`;\n",
    "- a `y` variant with shape `(number_of_pixels, )` ie a single column matrix where each cell indicates the target value for each pixel.\n",
    "\n",
    "Using the masks you can build for each stack of slice, you can easily filter and reshape the 2 arrays we need:\n",
    "- `train_data_x_sample`\n",
    "- `train_data_y_sample`\n",
    "\n",
    "**We will care about the validation set later as it does not need to be filtered as carefully.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Using the mask we created, filter (and reshape if needed) the array of observations and the array of expected results for the training sample you defined.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((174052, 4), (174052,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "train_data_x_sample_no_bg = train_data_x_sample[mask_train_data_sample, :] # No background\n",
    "train_data_y_sample_no_bg = train_data_y_sample[mask_train_data_sample]\n",
    "\n",
    "val_data_x_sample_no_bg = val_data_x_sample[mask_val_data_sample, :]\n",
    "val_data_y_sample_no_bg = val_data_y_sample[mask_val_data_sample]\n",
    "\n",
    "# we expect shapes like:\n",
    "# - (169144, 4) for train_data_x_sample\n",
    "# - (169144,) for train_data_y_sample\n",
    "\n",
    "train_data_x_sample_no_bg.shape, train_data_y_sample_no_bg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create, train and validate a linear, pixel-level classifier\n",
    "A linear SVC with $10000$ iterations should give you a good idea of what linear classification can give you.\n",
    "\n",
    "As a first evaluation metric, you can use the accuracy.\n",
    "\n",
    "Displaying the results is probably the best way to have an idea of what is going on, at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create a linear classifier.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.svm import LinearSVC\n",
    "svclassifier = LinearSVC(max_iter=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Train your linear classifier (it should take a minute or two on a modern CPU).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "svclassifier.fit(train_data_x_sample_no_bg, train_data_y_sample_no_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Evaluate the performance of your linear classifier on your sample validation set. Do not forget to reshape your arrays so the input array has a shape of `(num_pixels, 4)` and the array of expected outputs has a shape of `(num_pixels,)`. Use the `score()` method of your classifier for now.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9588291838352268"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: evaluate\n",
    "svclassifier.score(val_data_x_sample_no_bg, val_data_y_sample_no_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**The score is not very informative because the classes are imbalanced. Use other tools like `classification_report` or `confusion_matrix` to analyse your results. You will need to compute and store the predictions for your validation set.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the predictions\n",
    "val_data_y_pred_sample_no_bg = svclassifier.predict(val_data_x_sample_no_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[155741,   1938],\n",
       "       [  5386,  14828]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "confusion_matrix(val_data_y_sample_no_bg, val_data_y_pred_sample_no_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    157679\n",
      "           1       0.88      0.73      0.80     20214\n",
      "\n",
      "    accuracy                           0.96    177893\n",
      "   macro avg       0.93      0.86      0.89    177893\n",
      "weighted avg       0.96      0.96      0.96    177893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(classification_report(val_data_y_sample_no_bg, val_data_y_pred_sample_no_bg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Display some of the results obtained on the validation set. You need to reshape the predictions (the original slices are 240x240).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4d0657b01f4eab8eed20949dfe4b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: display some results\n",
    "val_data_y_pred_sample = np.zeros_like(val_data_y_sample)\n",
    "val_data_y_pred_sample[mask_val_data_sample] = val_data_y_pred_sample_no_bg\n",
    "interact_display(val_data_x_sample, val_data_y_sample, val_data_y_pred_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may obtain very different results here between runs because of sampling randomness, SGD randomness, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Are you satisfied with the performance of your classifier? Write down why below.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "The performance of this first classifier could still be improved significantly.\n",
    "Indeed, in this study, we are especially interested in the positive class (the class `1` in the `classification_report` above).\n",
    "But here, this classifier has only a 73% recall score for this class; which means that 27% of tumoral pixels are still not detected...\n",
    "Imagine a surgeon who leaves 27% of a tumour in the patient's brain after an operation...\n",
    "\n",
    "We need to improve this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use the appropriate performance metric\n",
    "At this point, it is very important to ask ourselves what is the criterion we want to optimize.\n",
    "\n",
    "Clearly, the fraction of non-tumoral pixels is not very informative, because the classes are strongly imbalanced.\n",
    "\n",
    "Instead, we suggest to focus on how well we detect each tumor:\n",
    "- Do we recover all its pixels?\n",
    "- Do we add noise to the detection?\n",
    "\n",
    "We will summarize both of these indicators in a single one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for grading\n",
    "We will use the [*Sørensen–Dice coefficient*](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to evaluate the quality of your predictions on the test set (with hidden ground truth).\n",
    "Given two sets, X and Y, it is defined as:\n",
    "$$\n",
    "Dice(X,Y) = \\frac{2 | X \\cap Y |}{|X| + |Y|}\n",
    "$$\n",
    "where $|X|$ and $|Y|$ are the cardinalities of the two sets.\n",
    "\n",
    "Its values range from $0$ (worst) to $1$ (best).\n",
    "\n",
    "In the medical image processing community, this indicator is best know as the \"Dice coefficient\", \"Dice score\", or simply \"Dice\".\n",
    "\n",
    "It is [equivalent to the F1-score](https://brenocon.com/blog/2012/04/f-scores-dice-and-jaccard-set-similarity/), and when applied to boolean data, using the definition of true positive (TP), false positive (FP), and false negative (FN), it can be written as:\n",
    "$$\n",
    "Dice = \\frac{2TP}{2TP+FP+FN}\n",
    "$$\n",
    "\n",
    "There are several ways to compute it, and we list some hereafter.\n",
    "\n",
    "#### a) Read it from the classification report\n",
    "Scikit-learn's `classification_report()` function outputs several valuable indicators.\n",
    "The one we are considering is the f1-score for class `1`, which is `0.83` in the following report (an example of the performance we obtained on our validation set when training a MLP classifier with a sample of features pooled over a 3x3 window, after post-processing):\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      1.00      0.99    547247\n",
    "           1       0.95      0.73      0.83     19193\n",
    "\n",
    "   micro avg       0.99      0.99      0.99    566440\n",
    "   macro avg       0.97      0.87      0.91    566440\n",
    "weighted avg       0.99      0.99      0.99    566440\n",
    "```\n",
    "\n",
    "#### b) Use the confusion matrix \n",
    "We can compute the Dice coefficient using the true/false positives/negatives provided by the confusion matrix (Scikit-learn's `confusion_matrix()`).\n",
    "\n",
    "#### c) Compute it directly\n",
    "This function is quite easy to code and it may allow us to add more tests to ensure we did our job correctly.\n",
    "We provide you, below, with a `dice_score` function which does this. **This is the function we will use to measure the performance of your classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def dice_score(y_true, y_pred):\n",
    "    '''Compute the Dice coefficient between true and predicted values.'''\n",
    "    for var, name in ((y_true, \"y_true\"), (y_pred, \"y_pred\")):\n",
    "        if not isinstance(var, np.ndarray):\n",
    "            raise ValueError(\"dice_score: `%s` must be a numpy array. Got object of type %s instead.\"\n",
    "                             % (name, type(var)))\n",
    "        if var.ndim != 1:\n",
    "            raise ValueError(\"dice_score: `%s` must be 1-dimensional. Got %s.ndim=%d instead.\"\n",
    "                             % (name, name, var.ndim))\n",
    "        if not (var.dtype == np.uint8 or var.dtype == np.bool):\n",
    "            warnings.warn(\"dice_score: `%s` should be either of type np.uint8 or np.bool. \"\n",
    "                          \"Got %s instead. \"\n",
    "                          \"Values will be projected on {0,1}.\"\n",
    "                          % (name, var.dtype))\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"dice_score: `y_true` and `y_pred` must have the same shape. \"\n",
    "                         \"Got y_true.shape=%s and y_pred.shape=%s instead.\"\n",
    "                         % (y_true.shape, y_pred.shape))\n",
    "    y_true_ = y_true != 0  # Project over {False,True}\n",
    "    y_pred_ = y_pred != 0\n",
    "    # The boolean union removes the duplicate intersection!\n",
    "    union = np.sum(y_true_ + y_pred_)\n",
    "    if union == 0:\n",
    "        warnings.warn(\"dice_score: neither `y_true` nor `y_pred` contain positive values.\"\n",
    "                      \" The score is ill-defined. Returning `1.` without much faith...\")\n",
    "        return 1.\n",
    "    inter = np.sum(y_true_ * y_pred_)\n",
    "    # We need to re-add the intersection or we get the Jaccard index\n",
    "    return 2 * inter / (union + inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8019469983775014"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try it!\n",
    "dice_score(val_data_y_sample_no_bg.reshape((-1,)), val_data_y_pred_sample_no_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Create, train and validate a non-linear SVM classifier\n",
    "The SVM with RBF kernel is a very classical non-linear classifier.\n",
    "\n",
    "Let's see how well it can perform on our samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create a non-linear SVM classifier.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "svclassifier = sklearn.svm.SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Reuse the code you wrote in the previous step to train and validate you classifier. Do not forget to display some results.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "svclassifier.fit(train_data_x_sample_no_bg, train_data_y_sample_no_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# compute the predictions\n",
    "val_data_y_pred_sample_no_bg = svclassifier.predict(val_data_x_sample_no_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    157679\n",
      "           1       0.87      0.74      0.80     20214\n",
      "\n",
      "    accuracy                           0.96    177893\n",
      "   macro avg       0.92      0.86      0.89    177893\n",
      "weighted avg       0.96      0.96      0.96    177893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(classification_report(val_data_y_sample_no_bg, val_data_y_pred_sample_no_bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f5759da62e4ca1b93b7b6ced9c97f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: display some results\n",
    "val_data_y_pred_sample = np.zeros_like(val_data_y_sample)\n",
    "val_data_y_pred_sample[mask_val_data_sample] = val_data_y_pred_sample_no_bg\n",
    "interact_display(val_data_x_sample, val_data_y_sample, val_data_y_pred_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should consistently get better results with this classifier (on this problem), but it is much slower.\n",
    "\n",
    "There are [two main parameters for a RBF SVM](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html): C and $\\gamma$ (gamma). It would perfectly make sense to perform a [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) of the best possible parameter pair. If you go this way, and we strongly encourage you to do so, make sure to use [all](https://scikit-learn.org/stable/modules/grid_search.html) [the](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) [goodies](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) Scikit-learn offers!\n",
    "\n",
    "But first, make sure you try the other options we suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6. Multi-layer Perceptron\n",
    "The MLP is a very powerful classifier, despite its strong tendency to overfit when used without care.\n",
    "\n",
    "There are very powerful libraries for training neural networks nowadays, and you should use them.\n",
    "\n",
    "However, for the sake of rapidity, we will use Scikit-learn's implementation for our first experiments. It uses a parallel CPU version and is easier than the majors libraries like [Keras](http://keras.io/), [Tensorflow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), [MS CNTK](https://docs.microsoft.com/en-gb/cognitive-toolkit/), [Apache MXNet](https://mxnet.incubator.apache.org/) and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create a non-linear MLP classifier, train it, and measure its performance on your validation sample.**\n",
    "</div>\n",
    "\n",
    "*Tips:*\n",
    "- use relu activations with a small regularization loss (`1e-6` or so);\n",
    "- use the \"adam\" solver with large batch size (we tried `256` with success);\n",
    "- use few hidden layers with few units to start (we tried with two layers of 16 and 4 units respectively);\n",
    "- as Scikit-learn's implementation integrate an early stopping mechanism by default, we can set the maximum number of iterations to a quite high number (like `100`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-06, batch_size=256, hidden_layer_sizes=(16, 4),\n",
       "              max_iter=100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "mlpclf1 = MLPClassifier(\n",
    "    hidden_layer_sizes=(16, 4),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-6,\n",
    "    batch_size=256,\n",
    "    max_iter=100\n",
    ")\n",
    "mlpclf1.fit(train_data_x_sample_no_bg, train_data_y_sample_no_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98    157679\n",
      "           1       0.84      0.82      0.83     20214\n",
      "\n",
      "    accuracy                           0.96    177893\n",
      "   macro avg       0.91      0.90      0.90    177893\n",
      "weighted avg       0.96      0.96      0.96    177893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: predict & report performance\n",
    "val_data_y_pred_sample_no_bg = mlpclf1.predict(val_data_x_sample_no_bg)\n",
    "print(classification_report(val_data_y_sample_no_bg, val_data_y_pred_sample_no_bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075a72148ad147b59d4b70606a6a660b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: display some results\n",
    "\n",
    "val_data_y_pred_sample = np.zeros_like(val_data_y_sample)\n",
    "val_data_y_pred_sample[mask_val_data_sample] = val_data_y_pred_sample_no_bg\n",
    "interact_display(val_data_x_sample, val_data_y_sample, val_data_y_pred_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7. Random Forests\n",
    "We could also try a Random Forest Classifier, but it you are short in time then just skip this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Benchmark a Random Forest Classifier if you have time..**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98    157679\n",
      "           1       0.84      0.75      0.79     20214\n",
      "\n",
      "    accuracy                           0.96    177893\n",
      "   macro avg       0.91      0.87      0.88    177893\n",
      "weighted avg       0.95      0.96      0.95    177893\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5892e01f9454f579b4d40d0b3861053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO train, compute predictions, compute score, display some results…\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(train_data_x_sample_no_bg, train_data_y_sample_no_bg)\n",
    "\n",
    "val_data_y_pred_sample_no_bg = forest.predict(val_data_x_sample_no_bg)\n",
    "print(classification_report(val_data_y_sample_no_bg, val_data_y_pred_sample_no_bg))\n",
    "    \n",
    "val_data_y_pred_sample = np.zeros_like(val_data_y_sample)\n",
    "val_data_y_pred_sample[mask_val_data_sample] = val_data_y_pred_sample_no_bg\n",
    "interact_display(val_data_x_sample, val_data_y_sample, val_data_y_pred_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7.1 Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98    157679\n",
      "           1       0.82      0.79      0.81     20214\n",
      "\n",
      "    accuracy                           0.96    177893\n",
      "   macro avg       0.90      0.88      0.89    177893\n",
      "weighted avg       0.96      0.96      0.96    177893\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2aa0e20133543d08e510ae7d14e4fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO train, compute predictions, compute score, display some results…\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboot = AdaBoostClassifier()\n",
    "adaboot.fit(train_data_x_sample_no_bg, train_data_y_sample_no_bg)\n",
    "\n",
    "val_data_y_pred_sample_no_bg = adaboot.predict(val_data_x_sample_no_bg)\n",
    "print(classification_report(val_data_y_sample_no_bg, val_data_y_pred_sample_no_bg))\n",
    "    \n",
    "val_data_y_pred_sample = np.zeros_like(val_data_y_sample)\n",
    "val_data_y_pred_sample[mask_val_data_sample] = val_data_y_pred_sample_no_bg\n",
    "interact_display(val_data_x_sample, val_data_y_sample, val_data_y_pred_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second best f1-score up to now. But foremost, best recall!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 8. Use more context for classification\n",
    "**This is a tedious part, and you should skip it if you do not have enough time.**\n",
    "*Note, however, that it should improve your results significantly.*\n",
    "\n",
    "Until now, we used only 4 values to predict the class of a pixel: the normalized intensity values for each modality at this exact pixel location.\n",
    "\n",
    "We can improve our results by looking around each pixel and using those values to compute the class of the central pixel.\n",
    "\n",
    "This is exactly what convolutional neural networks do: at each stage, the new features are computed by looking simultaneously at:\n",
    "- a neighborhood of pixels (centered around a particular pixel of interest);\n",
    "- the values of the features produced by the previous layer (or the input) for all those pixels.\n",
    "\n",
    "#### Sliding windows\n",
    "We will implement this by building a new training matrix with more features.\n",
    "We will proceed as follows. Given a certain window size, for all slices, we will compute a new slice where each pixel will have as features the features of all the pixels within the window around the pixel at the same coordinate in the original slice.\n",
    "\n",
    "It can be viewed as a virtual **sliding window** over the pixels of the original slice.\n",
    "\n",
    "The figure below is an attempt at explaining this idea visually, with a 3x3 window…\n",
    "![3x3 pooling](img/practice_06/window3x3.png)\n",
    "\n",
    "To accomplish this step, the Scikit-image function [`util.view_as_windows()`](https://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.view_as_windows) is very helpful.\n",
    "\n",
    "There is [an interesting discussion on SO](https://stackoverflow.com/questions/45327829/fastest-way-to-select-77-neighbor-pixels-for-every-pixel-in-an-image-in-python) about this.\n",
    "\n",
    "#### Padding\n",
    "The challenge with this approach is to handle the border of the slices: without any special care, the resulting slice will be smaller than the original one: its new shape will be `(orig_rows - win_size + 1, orig_cols - win_size + 1)`.\n",
    "\n",
    "We can either post-process the slices by adding the missing border, or pre-process the slices by adding them an extra border which will be removed afterward.\n",
    "\n",
    "Pre-processing the slices is actually simpler because it avoids having to perform complicated reshaping later on for the evaluation.\n",
    "\n",
    "To accomplish this step, the NumPy function [`np.pad()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html) is very helpful.\n",
    "\n",
    "#### About the ground truth\n",
    "The ground truth slices should keep the same shape.\n",
    "\n",
    "#### Masking\n",
    "Of course, **you still need to filter out the background pixels,** like we do since step 2.\n",
    "The good news is that the function `slices_to_masks()` we gave you can be applied on the resulting slices (with more than 4 dimensions). It may, however, remove a border from the brain…\n",
    "Any fix regarding this issue is welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Recompute a sample\n",
    "\n",
    "We need to recover our training sample with its original shape, using the same data as before to facilitate comparisons between the approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Recreate the training sample with shape `(n_slices, 240, 240, 4)` for x and `(n_slices, 240, 240)` for y.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# train_data_x_sample = ???\n",
    "# train_data_y_sample = ???\n",
    "# train_data_x_sample.shape, train_data_y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Padding, windowed view, reshaping\n",
    "\n",
    "First, we suggest you use a small window size to begin with, as the memory requirements grow in O(n²) with the size of your window.\n",
    "\n",
    "Second, we also suggest you carefully implement a function which will:\n",
    "- pad the original data;\n",
    "- build new slices by stacking features from neighbor pixels;\n",
    "- reshape the sample appropriately so its mask can be computed easily.\n",
    "\n",
    "To guide you throughout theses steps, we will help you performing them separately at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 3  # Must be odd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create a padded training sample, according to the `window_size` you defined.**\n",
    "</div>\n",
    "\n",
    "*Tips:*\n",
    "- You **must not** change the number of slices not the number of features.\n",
    "- You must add `(win_size - 1) // 2` pixels before the first row and columns, and after the last row and column of your array.\n",
    "- You should just copy the values at the edge as they should belong to the background.\n",
    "- If `train_data_x_sample.shape` is `(10, 240, 240, 4)` and `win_size = 3`, then `train_data_x_sample_padded.shape` must be `(10, 242, 242, 4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the training sample\n",
    "train_data_x_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pad your sample\n",
    "# train_data_x_sample_padded = np.pad(train_data_x_sample, ???)\n",
    "# train_data_x_sample_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Now use `skimage.util.view_as_windows` to view your training sample as a windowed image.**\n",
    "</div>\n",
    "\n",
    "*Tips:*\n",
    "- If `train_data_x_sample_padded.shape` is `(10, 242, 242, 4)` and `win_size = 3`, then `train_data_x_sample_win.shape` should be `(10, 240, 240, 1, 1, 3, 3, 4)`. This is a bit weird but easy to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# train_data_x_sample_win = util.view_as_windows(train_data_x_sample_padded, \n",
    "#                                                ???)\n",
    "# train_data_x_sample_win.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Fix the shape of `train_data_x_sample_win` so it has 4 dimensions: `(slices, rows, cols, features)`.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# train_data_x_sample_win = train_data_x_sample_win.reshape(???)\n",
    "# train_data_x_sample_win.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Now complete the function below which takes a training array, a window size, and returns a new array with features pooled locally over a sliding window (just make a function out of the separated steps you ran before).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# def pool_local_features(training_array, win_size):\n",
    "#     \"\"\"\n",
    "#     Creates a new array from a source array were information for each voxel is augmented with information of\n",
    "#     neighbor pixels.\n",
    "    \n",
    "#     Warning: a large `win_size` will fill you RAM very quickly!\n",
    "    \n",
    "#     Arguments\n",
    "#     ---------\n",
    "#     training_array: np.array of shape (num_slices, 240, 240, 4)\n",
    "#         Input array to transform, where each voxel (s, x, y) maps to a vector of 4 scalar values.\n",
    "    \n",
    "#     win_size: integer, must be odd\n",
    "#         Size of a size of the window. `3` will produce a 9 by 9 window.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     training_array_win: np.array of shape (num_slices, 240, 240, 4*win_size)\n",
    "#         Input array viewed as window.\n",
    "#         For each voxel (s, x, y) it maps to a vector of 4 * map_size scalar values,\n",
    "#         flattening vectors of neighbor pixels in the window.\n",
    "#     \"\"\"\n",
    "#     if training_array.ndim != 4:\n",
    "#         raise ValueError(\"pool_local_features: training_array must be 4-dimensional. \"\n",
    "#                          \"We got training_array.ndim=%d instead.\" % training_array.ndim)\n",
    "#     if training_array.shape[1:] != (240, 240, 4):\n",
    "#         raise ValueError(\"pool_local_features: training_array shape must be (num_slices, 240, 240, 4). \"\n",
    "#                          \"We got training_array.shape=%s instead.\" % (training_array.shape,))\n",
    "#     ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Compute the mask of the new training sample\n",
    "As said previously, we still need to filter the background pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute and apply the masks for each slices using the `slices_to_masks()` function we provided.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# mask_train_data_sample_win = ???\n",
    "# mask_train_data_sample_win.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# train_data_x_sample_win = ???  # apply mask\n",
    "# train_data_x_sample_win.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the ground truth for those observations. Thanks to the padding we just need to apply the filter to `train_data_y_sample`.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# train_data_y_sample_win = ???   # apply mask\n",
    "# train_data_y_sample_win.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Train again!\n",
    "You are now ready to train your favorite classifier one more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Retrain a MLP classifier (or another one as per your preferences) on this new sample. Do not forget to create another classifier to avoid reusing the previously-computed weights.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# mlpclf2 = MLPClassifier(???)\n",
    "# mlpclf2.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Evaluation\n",
    "To evaluate our classifier we need to perform the same transformation over our validation sample.\n",
    "\n",
    "That was the purpose of creating the `pool_local_features()` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the validation sample\n",
    "# val_data_x_sample_win = pool_local_features(val_data_x_sample, win_size)\n",
    "# val_data_x_sample_win.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the associated ground truth: no change required!\n",
    "val_data_y_sample_win = val_data_y_sample\n",
    "val_data_y_sample_win.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Now evaluate this new classifier.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# compute the predictions\n",
    "# val_data_ypred_sample = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# print(classification_report(...)) or dice_score or …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# display some results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get a significant improvement with this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Post-process the results\n",
    "**Note: this step brings you the last percents you can get, make sure you complete the previous steps before.**\n",
    "\n",
    "The results are a bit noisy and we have some prior knowledge on the structure of the regions we expect.\n",
    "\n",
    "We will improve the results by:\n",
    "1. filtering the small isolated pixels which probably are noise;\n",
    "2. fill small holes within large tumoral regions as they probably are missed elements.\n",
    "\n",
    "We can use simple morphological tools to perform those two steps.\n",
    "\n",
    "Scipy has two tools for this purpose:\n",
    "- `scipy.ndimage.binary_opening()`\n",
    "- `scipy.ndimage.binary_fill_holes`\n",
    "\n",
    "`binary_opening` needs a structuring element which can be created with `scipy.ndimage.generate_binary_structure()`.\n",
    "\n",
    "For both of those functions, the slices need to be processed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True, False, False],\n",
       "       [False,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [False,  True,  True,  True, False],\n",
       "       [False, False,  True, False, False]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "struct = np.array([\n",
    "    [ False,  False,  True,  False,  False],\n",
    "    [ False,  True,   True,  True,   False],\n",
    "    [ True,   True,   True,  True,   True ],\n",
    "    [ False,  True,   True,  True,   False],\n",
    "    [ False,  False,  True,  False,  False]\n",
    "])\n",
    "struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Using the morphology functions we previously mentioned, filter your results to improve them. You may need to reshape `val_data_ypred_sample`.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: filter your slices\n",
    "def post_process(val_data_y_pred_sample, struct):\n",
    "    slices = []\n",
    "    for slice_ in val_data_y_pred_sample:\n",
    "        slice_ = scipy.ndimage.binary_opening(slice_, structure=struct)\n",
    "        slice_ = scipy.ndimage.binary_fill_holes(slice_, structure=struct).astype(int) # Try with a struct also ?\n",
    "        slices.append(slice_)\n",
    "\n",
    "    val_data_y_pred_sample_morpho = np.dstack(slices)\n",
    "    val_data_y_pred_sample_morpho = np.moveaxis(val_data_y_pred_sample_morpho, -1, 0)\n",
    "    \n",
    "    return val_data_y_pred_sample_morpho\n",
    "\n",
    "val_data_y_pred_sample_morpho = post_process(val_data_y_pred_sample, struct)\n",
    "val_data_y_pred_sample_morpho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2f1c1cf38c4872b426ca3fe0075b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(i=(0, val_data_y_pred_sample.shape[0] - 1))\n",
    "def post_process_interact_display(i):\n",
    "    _, axis_arr = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "    axis_arr[0].imshow(val_data_y_pred_sample[i, ...])\n",
    "    axis_arr[0].axis(\"off\")\n",
    "    axis_arr[0].set_title(\"Original prediction\")\n",
    "    \n",
    "    axis_arr[1].imshow(val_data_y_pred_sample_morpho[i, ...])\n",
    "    axis_arr[1].axis(\"off\")\n",
    "    axis_arr[1].set_title(\"Post-Process prediction\")\n",
    "    \n",
    "    axis_arr[2].imshow(val_data_y_sample[i, ...])\n",
    "    axis_arr[2].axis(\"off\")\n",
    "    axis_arr[2].set_title(\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Measure the improvements of this post-processing step, and do not forget the check the results visually.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    157679\n",
      "           1       0.93      0.75      0.83     20214\n",
      "\n",
      "    accuracy                           0.97    177893\n",
      "   macro avg       0.95      0.87      0.91    177893\n",
      "weighted avg       0.97      0.97      0.96    177893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "val_data_y_pred_sample_morpho_no_bg = val_data_y_pred_sample_morpho[mask_val_data_sample]\n",
    "print(classification_report(val_data_y_sample_no_bg, val_data_y_pred_sample_morpho_no_bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e519fbf25eaa48a692485f7f1b7b938f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='i', max=9), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: display some results\n",
    "interact_display(val_data_x_sample, val_data_y_sample, val_data_y_pred_sample_morpho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train on complete dataset\n",
    "\n",
    "Training on the complete dataset should give us the best performance.\n",
    "However, it may also take a long time and require a lot of computing resources.\n",
    "Also, it is always a good idea to **keep a separate subset for validating** the performance of the predictor you trained.\n",
    "\n",
    "Make sure you run this on a machine with enough RAM.\n",
    "\n",
    "In a production environment, when using neural networks, we would use a dedicated library (like Keras and others), or a least carefully use the `warm_start` of Scikit-learn classifiers which support this option as it permit to learn incrementally on batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**If you have time, try a training on the full training set. Do not forget to validate the performance of your classifier!**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((205, 240, 240, 4), (205, 240, 240), (51, 240, 240, 4), (51, 240, 240))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: process the full dataset\n",
    "\n",
    "nb_sample = train_data_x.shape[0]\n",
    "train_percent = 0.8\n",
    "\n",
    "nb_train_sample = round(train_percent * nb_sample)\n",
    "nb_val_sample = round((1 - train_percent) * nb_sample)\n",
    "\n",
    "sample_id = rng.choice(train_data_x.shape[0], size=(nb_train_sample + nb_val_sample), replace=False)\n",
    "\n",
    "sample_train_id = sample_id[:nb_train_sample]\n",
    "sample_val_id = sample_id[nb_train_sample:]\n",
    "\n",
    "train_data_x_sample = train_data_x[sample_train_id]\n",
    "train_data_y_sample = train_data_y[sample_train_id]\n",
    "val_data_x_sample = train_data_x[sample_val_id]\n",
    "val_data_y_sample = train_data_y[sample_val_id]\n",
    "\n",
    "train_data_x_sample.shape, train_data_y_sample.shape, val_data_x_sample.shape, val_data_y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3523332, 4), (3523332,), (879243, 4), (879243,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_train_data_sample = slices_to_masks(train_data_x_sample)\n",
    "mask_val_data_sample = slices_to_masks(val_data_x_sample)\n",
    "\n",
    "train_data_x_sample_no_bg = train_data_x_sample[mask_train_data_sample, :] # No background\n",
    "train_data_y_sample_no_bg = train_data_y_sample[mask_train_data_sample]\n",
    "\n",
    "val_data_x_sample_no_bg = val_data_x_sample[mask_val_data_sample, :]\n",
    "val_data_y_sample_no_bg = val_data_y_sample[mask_val_data_sample]\n",
    "\n",
    "train_data_x_sample_no_bg.shape, train_data_y_sample_no_bg.shape, val_data_x_sample_no_bg.shape, val_data_y_sample_no_bg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    776252\n",
      "           1       0.91      0.68      0.77    102991\n",
      "\n",
      "    accuracy                           0.95    879243\n",
      "   macro avg       0.93      0.83      0.87    879243\n",
      "weighted avg       0.95      0.95      0.95    879243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(16, 4),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-6,\n",
    "    batch_size=256,\n",
    "    max_iter=100,\n",
    "    warm_start=True\n",
    ")\n",
    "best_classifier.fit(train_data_x_sample_no_bg, train_data_y_sample_no_bg)\n",
    "\n",
    "val_data_y_pred_sample_no_bg = best_classifier.predict(val_data_x_sample_no_bg)\n",
    "print(classification_report(val_data_y_sample_no_bg, val_data_y_pred_sample_no_bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97    776252\n",
      "           1       0.97      0.62      0.76    102991\n",
      "\n",
      "    accuracy                           0.95    879243\n",
      "   macro avg       0.96      0.81      0.87    879243\n",
      "weighted avg       0.96      0.95      0.95    879243\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086df37c14e6479d94502f22f82bb43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='i', max=50), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data_y_pred_sample = np.zeros_like(val_data_y_sample)\n",
    "val_data_y_pred_sample[mask_val_data_sample] = val_data_y_pred_sample_no_bg\n",
    "val_data_y_pred_sample_morpho =  post_process(val_data_y_pred_sample, struct)\n",
    "\n",
    "val_data_y_pred_sample_morpho_no_bg = val_data_y_pred_sample_morpho[mask_val_data_sample]\n",
    "print(classification_report(val_data_y_sample_no_bg, val_data_y_pred_sample_morpho_no_bg))\n",
    "\n",
    "interact_display(val_data_x_sample, val_data_y_sample, val_data_y_pred_sample_morpho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Last training with **EVERYTHING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-06, batch_size=256, hidden_layer_sizes=(16, 4),\n",
       "              max_iter=100, warm_start=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier.fit(val_data_x_sample_no_bg, val_data_y_sample_no_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Process the test set\n",
    "Using the best classifier among the ones you trained, process the test set which will be used for grading.\n",
    "\n",
    "Remember to:\n",
    "- prepare data (especially if you pool features over local windows);\n",
    "- predict the classes;\n",
    "- apply post processing;\n",
    "- control them visually!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Process the test set.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b511bd723a4945718a7bd4d35b6683b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=14, description='i', max=28), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO perform all the required steps\n",
    "# Data load & preparation\n",
    "test_data_x = np.memmap(PATH_TO_RESOURCES / \"test_data_x.dat\", mode=\"r\", dtype=np.float32, shape=(29, 240, 240, 4))\n",
    "mask_test_data = slices_to_masks(test_data_x)\n",
    "test_data_x_no_bg = test_data_x[mask_test_data]\n",
    "\n",
    "# prediction\n",
    "test_data_y_pred_no_bg = best_classifier.predict(test_data_x_no_bg)\n",
    "\n",
    "# post processing\n",
    "test_data_y_pred = np.zeros(test_data_x.shape[:3])\n",
    "test_data_y_pred[mask_test_data] = test_data_y_pred_no_bg\n",
    "test_data_y_pred_morpho =  post_process(test_data_y_pred, struct).astype(np.uint8)\n",
    "\n",
    "# control\n",
    "interact_display(test_data_x, test_data_y_pred_morpho, chan_names = [\"T1\", \"T1ce\", \"T2\", \"FLAIR\", \"PRED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export your results and submit them for grading\n",
    "**This is the most important part!**\n",
    "\n",
    "Using the function we provide below, check and export your results, then submit them for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_test_predictions(test_ypred, filename='test_data_ypred.dat'):\n",
    "    expected_shape = (29, 240, 240)\n",
    "    expected_dtype = np.uint8\n",
    "    # Check shape\n",
    "    if test_ypred.shape != expected_shape:\n",
    "        raise ValueError(\"export_test_predictions: your prediction do not have the right shape.\"\n",
    "                         \" Expected shape %s but got %s instead.\" % (expected_shape, test_ypred.shape))\n",
    "    # Check dtype\n",
    "    if test_ypred.dtype != expected_dtype:\n",
    "        raise ValueError(\"export_test_predictions: your prediction do not have the right dtype.\"\n",
    "                         \" Expected dtype %s but got %s instead.\" % (expected_dtype, test_ypred.dtype))\n",
    "    # Check values\n",
    "    if np.sum(test_ypred == 1) + np.sum(test_ypred == 0) != np.prod(expected_shape):\n",
    "        raise ValueError(\"export_test_predictions: your prediction should contain only binary (0 and 1)\"\n",
    "                         \" values. It seems that you have other values.\")\n",
    "    # Seems ok\n",
    "    test_data_ypred = np.memmap(filename, mode=\"w+\", dtype=expected_dtype, shape=expected_shape)\n",
    "    test_data_ypred[...] = test_ypred  # copy values\n",
    "    # The buffer is automatically flushed on disk upon object destruction but can also be explicit\n",
    "    test_data_ypred.flush()\n",
    "    print(\"Wrote '%s'.\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 'test_data_ypred.dat'.\n"
     ]
    }
   ],
   "source": [
    "export_test_predictions(test_data_y_pred_morpho.reshape((29, 240, 240)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job done!\n",
    "Thanks for attending MLRF.\n",
    "\n",
    "We hope you enjoyed solving those sessions as much as we did preparing them."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
